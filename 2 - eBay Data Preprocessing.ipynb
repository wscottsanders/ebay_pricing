{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - eBay Data Preprocessing\n",
    "\n",
    "### Workflow for Preprocessing\n",
    "- Create average hash of images to detect duplicates.\n",
    "- Calculate an image \"uniqueness\" score based on frequency of duplicates. \n",
    "- Standardize prices based on items MSRP. \n",
    "- Calculate a description uniqueness score based on item's description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imagehash\n",
    "import mysql.connector\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Image Hashes  \n",
    "\n",
    "One of our hypotheses in this study is that the uniqueness of information \n",
    "will affect the final sale price. Thus, an average hash of the image is \n",
    "created that allows use to programmatically identify duplicates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def connect_to_mysql(user, pw, host, database):\n",
    "    '''Creates connection with mysql database and returns cnx & cursor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user : str\n",
    "        MySQL user name.\n",
    "    pw : str\n",
    "        MySQL password. \n",
    "    host : str\n",
    "        MySQL hostname or IP address. \n",
    "    database : str\n",
    "        MySQL database name. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cnx \n",
    "        MySQL connection object.\n",
    "    cursor\n",
    "        MySQL cursor object.    \n",
    "    '''\n",
    "    cnx = mysql.connector.connect(user=user, password=pw,\n",
    "                                  host=host, database=database)\n",
    "    cursor = cnx.cursor()\n",
    "    return cnx, cursor\n",
    "\n",
    "\n",
    "def retrieve_from_mysql(query, user, pw, host, db):\n",
    "    \"\"\"Retrieves data from MySQL database.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str \n",
    "        Valid MySQL query.  \n",
    "    user : str\n",
    "        MySQL user name.\n",
    "    pw : str\n",
    "        MySQL password. \n",
    "    host : str\n",
    "        MySQL hostname or IP address. \n",
    "    database : str\n",
    "        MySQL database name.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : lst of tuples \n",
    "        Query results in list of tuples. \n",
    "    \"\"\"\n",
    "    cnx, cursor = connect_to_mysql(user, pw, host, db)\n",
    "\n",
    "    cursor.execute(query)\n",
    "\n",
    "    data = cursor.fetchall()\n",
    "    cnx.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_config_file(file_name):\n",
    "    '''Takes json file name as str, returns dict of file contents.\n",
    "       Used in Python2 to convert unicode json to ascii. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Json filename.        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict \n",
    "        A dictionary of with str in ASCII.  \n",
    "    '''\n",
    "\n",
    "    def ascii_encode_dict(data):\n",
    "        ''' https://stackoverflow.com/q/9590382  \n",
    "        '''\n",
    "\n",
    "        def ascii_encode(x): return x.encode(\n",
    "            'ascii') if isinstance(x, unicode) else x\n",
    "        return dict(map(ascii_encode, pair) for pair in data.items())\n",
    "\n",
    "    with open(file_name, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    return json.loads(json.dumps(config), object_hook=ascii_encode_dict)\n",
    "\n",
    "\n",
    "def add_hashes_to_mysql(data):\n",
    "    \"\"\"Adds hashed image data to MySQL database.   \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict \n",
    "        Dict of data from hash_dict() and dimensions() functions. \n",
    "    Returns\n",
    "    -------\n",
    "    None \n",
    "    \"\"\"\n",
    "\n",
    "    add_imagehash = (\"INSERT INTO image_hash\"\n",
    "                     \"(itemId, hash, mirror, left_hash, right_hash, width, height)\"\n",
    "                     \"VALUES (%s, %s, %s, %s, %s, %s, %s)\")\n",
    "\n",
    "    imagehash_data = (data['itemId'],\n",
    "                      str(data['hash']),\n",
    "                      str(data['mirror']),\n",
    "                      str(data['left']),\n",
    "                      str(data['right']),\n",
    "                      data['width'],\n",
    "                      data['height'])\n",
    "\n",
    "    try:\n",
    "        cursor.execute(add_imagehash, imagehash_data)\n",
    "        cnx.commit()\n",
    "\n",
    "    except:\n",
    "        print \"Failed to add hash for item {}.\".format(data['itemId'])\n",
    "        print '-----------------------'\n",
    "        print ''\n",
    "\n",
    "\n",
    "def hash_dict(im):\n",
    "    \"\"\"Returns average hash for different orientations. \n",
    "       Requires pillow, imagehash.   \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im : str or Image object \n",
    "        Path to file or a pil image object. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hashes: dict \n",
    "        Dictionary of average hashes of images with variations. \n",
    "        Generate hashes for the image, including variations of the image:\n",
    "            * hash: Regular image\n",
    "            * mirror: Mirrored (left-right)\n",
    "            * left: Rotated left (90deg)\n",
    "            * right: Rotated right (270deg)\n",
    "    \"\"\"\n",
    "    # Open image if not PIL image object.\n",
    "    if not isinstance(im, Image.Image):\n",
    "        im = Image.open(im)\n",
    "\n",
    "    # Resize, downsample, and convert to greyscale.\n",
    "    im = im.resize((16, 16), Image.ANTIALIAS).convert('L')\n",
    "\n",
    "    hashes = {}\n",
    "\n",
    "    # Regular hash\n",
    "    hashes['hash'] = imagehash.average_hash(im)\n",
    "\n",
    "    # Mirror hash\n",
    "    mirror_im = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    hashes['mirror'] = imagehash.average_hash(mirror_im)\n",
    "\n",
    "    # Rotated 90deg hash\n",
    "    left_im = im.transpose(Image.ROTATE_90)\n",
    "    hashes['left'] = imagehash.average_hash(left_im)\n",
    "\n",
    "    # Rotated 270deg hash\n",
    "    right_im = im.transpose(Image.ROTATE_270)\n",
    "    hashes['right'] = imagehash.average_hash(right_im)\n",
    "\n",
    "    del im\n",
    "    return hashes\n",
    "\n",
    "\n",
    "def dimensions(im):\n",
    "    \"\"\"Returns image dimensions (helper). \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    im : str or Image object \n",
    "        Path to file or a pil image object. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    hashes: dict\n",
    "        Dict of image dimensions. \n",
    "            * width: image width\n",
    "            * height: image height \n",
    "    \"\"\"\n",
    "    if not isinstance(im, Image.Image):\n",
    "        im = Image.open(im)\n",
    "\n",
    "    dims = {}\n",
    "    dims['width'] = im.size[0]\n",
    "    dims['height'] = im.size[1]\n",
    "    del im\n",
    "    return dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Example usage of above functions.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Read config for credentials.\n",
    "    secrets = read_config_file('secrets.json')\n",
    "\n",
    "    # Initiate mysql connection.\n",
    "    cnx, cursor = connect_to_mysql(secrets['user'], secrets['password'],\n",
    "                                   secrets['host'], secrets['database'])\n",
    "\n",
    "    # Create a list of image files to hash.  Assumes running from same\n",
    "    # folder as notebook 1.\n",
    "    dir_path = os.path.join(os.getcwd(), 'ebay_listing_photos')\n",
    "\n",
    "    image_list = [file for file in os.listdir(dir_path)\n",
    "                  if file.lower().endswith((\".jpg\", \".png\", \".bmp\"))]\n",
    "\n",
    "    # Don't hash previously hashed images.\n",
    "    query = \"SELECT DISTINCT(itemId) from image_hash\"\n",
    "    prev_hashed = [i[0] for i in retrieve_from_mysql(query,\n",
    "                                                     secrets['user'],\n",
    "                                                     secrets['password'],\n",
    "                                                     secrets['host'],\n",
    "                                                     secrets['database'])]\n",
    "\n",
    "    new_images = [image for image in image_list\n",
    "                  if image.split('.')[0] not in prev_hashed]\n",
    "\n",
    "    # Hash images and add to MySQL.\n",
    "    for image in new_images:\n",
    "        image_path = os.path.join(dir_path, image)\n",
    "        image_data['itemId'] = image.split('.')[0]\n",
    "        image_data.update(hash_dict(image_path))\n",
    "        image_data.update(dimensions(image_path))\n",
    "        add_hashes_to_mysql(image_data)\n",
    "\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Image Uniqueness \n",
    "\n",
    "Now that we've hashed our images we can now identify duplicates based on the hash values.  Although it's possible to make this as complex as we like (e.g. comparing various orientations, calculating hamming distances for near matches) we're simply going to look for exact duplicates with the original image orientation. After all, most listings for a product should have *fairly* similar images.  \n",
    "\n",
    "Finally, the raw counts would be misleading as some products appear more frequently in the dataset than others.  Therefore, we'll divide the image frequency by the number of times its respective product appeared in the dataset to normalize the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hash, name, and itemId from MySQL.\n",
    "cnx, _ = connect_to_mysql(secrets['user'], secrets['password'],\n",
    "                          secrets['host'], secrets['database'])\n",
    "\n",
    "query = \"SELECT item_finding_api.itemId, item_finding_api.name, \" \\\n",
    "        \"image_hash.hash \" \\\n",
    "        \"from item_finding_api \" \\\n",
    "        \"Right JOIN image_hash \" \\\n",
    "        \"ON (item_finding_api.itemId=image_hash.itemId);\"\n",
    "\n",
    "df = pd.read_sql(query, con=cnx)\n",
    "cnx.close()\n",
    "\n",
    "# Calculate image uniqueness.\n",
    "df2 = df.groupby(['name', 'hash']).size().reset_index(name='image_count')\n",
    "a = df2.groupby('name')['image_count'].transform('sum')\n",
    "df2['image_uniqueness'] = 1 - df2['image_count'].div(a)\n",
    "df2.drop('name', axis=1, inplace=True)\n",
    "df_hash = pd.merge(df, df2, on='hash', how='left')\n",
    "df_hash.drop('name', axis=1, inplace=True)\n",
    "\n",
    "del df, df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Prices \n",
    "\n",
    "When examining a diverse set of products it is not meaningful to calculate an average price across all products in the dataset.  Instead, we'll use the method described by [Dimoka, Pavlou, and Hong](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1976541) to calculate a price premium by subtracting the items MSRP from the Final Price and then dividing it by the MSRP and multiplying it by -1 to create a standardized value representing the percentage of the final selling price below the MSRP: \n",
    "\n",
    "\\begin{equation*}\n",
    "Discount = \\frac{(Price-MSRP)}{(MSRP)}* -1 \n",
    "\\end{equation*}\n",
    "\n",
    "The MSRP for each product was determined by visiting the website of the manufacturer, publisher, or studio and noting their suggested retail prices. MSRPs are appropriate benchmark prices for this type of calculation because although products are often not sold at the MSRP, their purpose is to serve as an anchoring price to prevent retailers from over or underpricing items for sale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "msrp = {'GTA 5 - Xbox': 59.99,\n",
    "        'windows 7 professional': 133.99,\n",
    "        'Bose Quietcomfort 15': 299.95,\n",
    "        'fifty shades of grey trilogy': 47.85,\n",
    "        'rayban wayfarer 2140 901': 155.00,\n",
    "        'Avengers - DVD': 17.95,\n",
    "        'WD Blue HDD': 54.99,\n",
    "        'Coco Mademoiselle (Eau de Parfum)': 122.00\n",
    "        }\n",
    "\n",
    "cnx, _ = connect_to_mysql(secrets['user'], secrets['password'],\n",
    "                          secrets['host'], secrets['database'])\n",
    "\n",
    "query = 'SELECT itemId, name, saleAmount from item_finding_api '\n",
    "\n",
    "df_disc = pd.read_sql(query, con=cnx)\n",
    "cnx.close()\n",
    "\n",
    "df_disc['msrp'] = df_disc['name'].map(msrp)\n",
    "\n",
    "df_disc['discount'] = ((df_disc['saleAmount'] - df_disc['msrp'])\n",
    "                       / df_disc['msrp'])*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search vs. Experience Goods \n",
    "\n",
    "We're going to quickly map the search and experience good categories to our \n",
    "product listings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_exp = {'GTA 5 - Xbox': 'search',\n",
    "              'windows 7 professional': 'search',\n",
    "              'Bose Quietcomfort 15': 'experience',\n",
    "              'fifty shades of grey trilogy': 'search',\n",
    "              'rayban wayfarer 2140 901': 'experience',\n",
    "              'Avengers - DVD': 'search',\n",
    "              'WD Blue HDD': 'search',\n",
    "              'Coco Mademoiselle (Eau de Parfum)': 'experience'\n",
    "              }\n",
    "\n",
    "df_disc['search_exp'] = df_disc['name'].map(search_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Description Uniqueness \n",
    "Description uniqueness represents the degree to which an item listings textual description was similar or different from other descriptions for the same product. First, for each product a matrix of cosine similarity scores was calculated by comparing the descriptions for each pair of listings. Cosine similarity is a metric used in information retrieval that allows for the comparison of vectors representing the frequency with which terms appear in a document. Given two vectors x and y,\n",
    "\n",
    "\\begin{equation}\n",
    "\\cos ({\\bf x},{\\bf y})= {{\\bf x} {\\bf y} \\over \\|{\\bf x}\\| \\|{\\bf y}\\|}\n",
    "\\end{equation}\n",
    "\n",
    "The cosine similarities were further converted into distance or dissimilarity scores as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ dist ({\\bf x},{\\bf y})= 1-cos ({\\bf x},{\\bf y})\n",
    "\\end{equation}\n",
    "\n",
    "The resulting score represents the degree to which a pair of listings were similar. Values that equal 0 indicated that the compared descriptions were identical while values approaching 1 indicated that the descriptions were quite different. Next, the matrix rows were averaged to create an aggregate uniqueness score representing how much a particular item description resembled all other item descriptions for the same product. Listings that did not include a text description, and thus resulted in a null value (N=232), were excluded when calculating the matrix row averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve descriptions from MySQL\n",
    "cnx, cursor = connect_to_mysql(secrets['user'], secrets['password'],\n",
    "                               secrets['host'], secrets['database'])\n",
    "\n",
    "query = \"SELECT item_shopping_api.itemId, item_finding_api.name, \" \\\n",
    "        \"item_shopping_api.Description \" \\\n",
    "        \"from item_shopping_api \" \\\n",
    "        \"LEFT JOIN item_finding_api \" \\\n",
    "        \"ON (item_shopping_api.itemId = item_finding_api.itemId) \" \\\n",
    "        \"WHERE item_shopping_api.Description is not Null\"\n",
    "\n",
    "data = retrieve_from_mysql(query, secrets['user'], secrets['password'],\n",
    "                           secrets['host'], secrets['database'])\n",
    "\n",
    "cnx.close()\n",
    "\n",
    "# Sort by product prior to calculating description uniqueness.\n",
    "sorted_by_product = {}\n",
    "\n",
    "for listing in data:\n",
    "    if listing[1] not in sorted_by_product:\n",
    "        sorted_by_product[listing[1]] = [listing]\n",
    "    else:\n",
    "        sorted_by_product[listing[1]].append(listing)\n",
    "\n",
    "# Calculate description uniqueness score by product.\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "uniqueness = []\n",
    "\n",
    "for key, value in sorted_by_product.iteritems():\n",
    "    # Create an index of itemIds.\n",
    "    ids = []\n",
    "    descriptions = []\n",
    "    for i in value:\n",
    "        ids.append(i[0]), descriptions.append(i[2])\n",
    "\n",
    "    tfidf = TfidfVectorizer().fit_transform(descriptions)\n",
    "    avg_cos_dist = cosine_distances(tfidf).mean(axis=0)\n",
    "    uniqueness += zip(ids, avg_cos_dist)\n",
    "\n",
    "df_u = pd.DataFrame(uniqueness, columns=['itemId', 'text_uniqueness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Data and Writing to File \n",
    "\n",
    "Finally, we're just going to do a bit of cleanup.  We're going to grab a few more interesting variables from MySQL, join them with our new calculated variables, and then we'll write the data to file for analysis in another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx, _ = connect_to_mysql(secrets['user'], secrets['password'],\n",
    "                          secrets['host'], secrets['database'])\n",
    "\n",
    "query = \"Select item_finding_api.itemId, item_finding_api.bidCount, \" \\\n",
    "        \"listing.startTime, listing.endTime, item_shopping_api.HitCount, \" \\\n",
    "        \"seller.feedbackScore, content_coding.content_coding \" \\\n",
    "        \"from item_finding_api \" \\\n",
    "        \"LEFT JOIN listing \" \\\n",
    "        \"ON (item_finding_api.itemId = listing.itemId) \" \\\n",
    "        \"LEFT JOIN item_shopping_api \" \\\n",
    "        \"ON (item_finding_api.itemId = item_shopping_api.itemId) \" \\\n",
    "        \"LEFT JOIN seller \" \\\n",
    "        \"ON (item_finding_api.itemId = seller.itemId) \" \\\n",
    "        \"LEFT JOIN content_coding \" \\\n",
    "        \"ON (item_finding_api.itemId = content_coding.itemId) \" \\\n",
    "        \"WHERE item_shopping_api.Description is not Null \" \\\n",
    "        \"AND content_coding.content_coding <> '' \" \\\n",
    "        \"AND content_coding.content_coding <> 'No Photo Provided';\"\n",
    "\n",
    "df = pd.read_sql(query, con=cnx)\n",
    "cnx.close()\n",
    "\n",
    "dfs = [df, df_u, df_hash, df_disc]\n",
    "df_final = reduce(lambda left, right: pd.merge(left, right, on='itemId'), dfs)\n",
    "df_final.to_csv('ebay_data.csv', header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ebay)",
   "language": "python",
   "name": "ebay_w"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
